{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0703733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import c\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import jieba\n",
    "\n",
    "class SFTDatasetBuilder:\n",
    "    def __init__(self, txt_folder_path: str, output_path: str = \"./sft_dataset\"):\n",
    "        \"\"\"\n",
    "        初始化数据集构建器\n",
    "        \n",
    "        Args:\n",
    "            txt_folder_path: 包含txt文件的文件夹路径\n",
    "            output_path: 输出数据集的路径\n",
    "        \"\"\"\n",
    "        self.txt_folder_path = txt_folder_path\n",
    "        self.output_path = output_path\n",
    "        self.context_min_length = 50  # context最小长度\n",
    "        self.context_max_length = 200  # context最大长度\n",
    "        self.text_min_length = 50  # text最小长度\n",
    "        self.text_max_length = 100  # text最大长度\n",
    "        self.positive_negative_ratio = 1.0  # 正负样本比例 1:1\n",
    "        \n",
    "        # 创建输出目录\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "    def parse_txt_file(self, file_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        解析单个txt文件，提取视频标题、正文内容和广告内容\n",
    "        \n",
    "        Args:\n",
    "            file_path: txt文件路径\n",
    "            \n",
    "        Returns:\n",
    "            dict: 包含标题、正文段落、广告段落、全文\n",
    "        \"\"\"\n",
    "        # 从文件名提取视频标题\n",
    "        video_title = Path(file_path).stem\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 按行分割内容\n",
    "        lines = content.strip().split('\\n')\n",
    "        \n",
    "        text_segments = []  # 正文段落\n",
    "        ad_segments = []    # 广告段落\n",
    "        current_segment = []\n",
    "        is_ad = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # 检查是否是广告开始\n",
    "            if line.startswith('- '):\n",
    "                # 保存之前的段落\n",
    "                if current_segment:\n",
    "                    if is_ad:\n",
    "                        ad_segments.append('\\n'.join(current_segment))\n",
    "                    else:\n",
    "                        text_segments.append('\\n'.join(current_segment))\n",
    "                    current_segment = []\n",
    "                \n",
    "                # 开始新的广告段落\n",
    "                is_ad = True\n",
    "                current_segment.append(line[2:])  # 移除'- '前缀\n",
    "            else:\n",
    "                # 如果之前是广告，现在不是，说明广告结束\n",
    "                if is_ad and not line.startswith('- '):\n",
    "                    if current_segment:\n",
    "                        ad_segments.append('\\n'.join(current_segment))\n",
    "                        current_segment = []\n",
    "                    is_ad = False\n",
    "                \n",
    "                current_segment.append(line)\n",
    "        \n",
    "        # 处理最后一个段落\n",
    "        if current_segment:\n",
    "            if is_ad:\n",
    "                ad_segments.append('\\n'.join(current_segment))\n",
    "            else:\n",
    "                text_segments.append('\\n'.join(current_segment))\n",
    "        \n",
    "        return {\n",
    "            'title': video_title,\n",
    "            'text_segments': text_segments,\n",
    "            'ad_segments': ad_segments,\n",
    "            'full_content': content\n",
    "        }\n",
    "    \n",
    "    def get_context_before_position(self, full_text: str, position: int) -> str:\n",
    "        \"\"\"\n",
    "        获取指定位置之前的上下文，保持文本连贯性\n",
    "        \n",
    "        Args:\n",
    "            full_text: 完整文本\n",
    "            position: 目标位置\n",
    "            \n",
    "        Returns:\n",
    "            上下文文本\n",
    "        \"\"\"\n",
    "        if position <= 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # 获取position之前的文本\n",
    "        before_text = full_text[:position].strip()\n",
    "        \n",
    "        if len(before_text) <= self.context_min_length:\n",
    "            return before_text\n",
    "        \n",
    "        # 如果超过最大长度，从句号处截断\n",
    "        if len(before_text) > self.context_max_length:\n",
    "            # 找到最后一个句号的位置\n",
    "            sentences = re.split(r'[。！？]', before_text)\n",
    "            if len(sentences) > 1:\n",
    "                # 保留到倒数第二个句子的结尾\n",
    "                truncated = '。'.join(sentences[:-1])\n",
    "                if len(truncated) >= self.context_min_length:\n",
    "                    return truncated + '。'\n",
    "            \n",
    "            # 如果没有合适的句号，直接截断到最大长度\n",
    "            return before_text[-self.context_max_length:]\n",
    "        \n",
    "        return before_text\n",
    "    \n",
    "    def split_text_into_chunks(self, \n",
    "                               text: str, \n",
    "                               max_length: int, \n",
    "                               min_length: int\n",
    "                               ) -> List[str]:\n",
    "        \"\"\"\n",
    "        将文本分割成合适长度的块，保持完整性\n",
    "        \n",
    "        Args:\n",
    "            text: 输入文本\n",
    "            \n",
    "        Returns:\n",
    "            文本块列表\n",
    "        \"\"\"\n",
    "        if len(text) <= self.text_max_length:\n",
    "            return [text] if len(text) >= self.text_min_length else []\n",
    "        \n",
    "        chunks = []\n",
    "        sentences = re.split(r'([。！？])', text)\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        for i in range(0, len(sentences), 2):\n",
    "            if i + 1 < len(sentences):\n",
    "                sentence = sentences[i] + sentences[i + 1]\n",
    "            else:\n",
    "                sentence = sentences[i]\n",
    "            \n",
    "            if len(current_chunk + sentence) <= self.text_max_length:\n",
    "                current_chunk += sentence\n",
    "            else:\n",
    "                if len(current_chunk) >= self.text_min_length:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = sentence\n",
    "        \n",
    "        if len(current_chunk) >= self.text_min_length:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def create_system_prompt(self) -> str:\n",
    "        \"\"\"创建系统提示\"\"\"\n",
    "        return \"\"\"你是一个专业的视频内容分析助手。你的任务是判断给定的文本片段是否为广告内容。\n",
    "\n",
    "广告内容通常具有以下特征：\n",
    "1. 包含商品推广、品牌宣传信息\n",
    "2. 包含购买链接、优惠信息、促销活动\n",
    "3. 包含\"点击链接\"、\"立即购买\"、\"限时优惠\"等营销用语\n",
    "4. 与视频主题内容无关或关联性较弱\n",
    "\n",
    "请根据视频标题和上下文内容，判断待分析文本是否为广告。只回答\"是\"或\"否\"。\"\"\"\n",
    "    \n",
    "    def create_user_prompt(self, video_title: str, context: str, text: str) -> str:\n",
    "        \"\"\"创建用户提示\"\"\"\n",
    "        return f\"\"\"视频标题：{video_title}\n",
    "\n",
    "上下文内容：\n",
    "{context}\n",
    "\n",
    "待分析文本：\n",
    "{text}\n",
    "\n",
    "请判断待分析文本是否为广告内容：\"\"\"\n",
    "    \n",
    "    def generate_samples_from_file(self, file_data: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从单个文件数据生成训练样本\n",
    "        \n",
    "        Args:\n",
    "            file_data: 解析后的文件数据\n",
    "            \n",
    "        Returns:\n",
    "            样本列表\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        full_content = file_data['full_content']\n",
    "        title = file_data['title']\n",
    "        # 生成正样本（广告）\n",
    "        for ad_segment in file_data['ad_segments']:\n",
    "            # 找到广告在全文中的位置\n",
    "            ad_position = full_content.find(ad_segment)\n",
    "            if ad_position == -1:\n",
    "                continue\n",
    "            \n",
    "            # 获取上下文\n",
    "            context = self.get_context_before_position(full_content, ad_position)\n",
    "            # 分割正文文本\n",
    "            context_chunks = self.split_text_into_chunks(context, self.context_max_length, self.context_min_length)\n",
    "            # 分割广告文本\n",
    "            ad_chunks = self.split_text_into_chunks(ad_segment, self.text_max_length, self.text_min_length)\n",
    "            for chunk_context in context_chunks:\n",
    "                for chunk_ad in ad_chunks:\n",
    "                    sample = {\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": self.create_system_prompt()},\n",
    "                            {\"role\": \"user\", \"content\": self.create_user_prompt(title, chunk_context, chunk_ad)},\n",
    "                            {\"role\": \"assistant\", \"content\": \"是\"}\n",
    "                        ]\n",
    "                    }\n",
    "                    samples.append(sample)\n",
    "        \n",
    "        # 生成负样本（非广告）\n",
    "        negative_count_needed = int(len([s for s in samples]) * self.positive_negative_ratio)\n",
    "        negative_samples = []\n",
    "        \n",
    "        for text_segment in file_data['text_segments']:\n",
    "            # 找到正文在全文中的位置\n",
    "            text_position = full_content.find(text_segment)\n",
    "            if text_position == -1:\n",
    "                continue\n",
    "            \n",
    "            # 分割正文文本\n",
    "            text_chunks = self.split_text_into_chunks(text_segment, self.text_max_length, self.text_min_length)\n",
    "            \n",
    "            for chunk in text_chunks:\n",
    "                # 如果负样本数量已经达到要求，则停止循环，防止生成过多的负样本\n",
    "                if len(negative_samples) >= negative_count_needed: \n",
    "                    break\n",
    "                \n",
    "                # 找到这个chunk在全文中的位置\n",
    "                chunk_position = full_content.find(chunk, text_position)\n",
    "                if chunk_position == -1:\n",
    "                    continue\n",
    "                \n",
    "                context = self.get_context_before_position(full_content, chunk_position)\n",
    "                \n",
    "                sample = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": self.create_system_prompt()},\n",
    "                        {\"role\": \"user\", \"content\": self.create_user_prompt(title, context, chunk)},\n",
    "                        {\"role\": \"assistant\", \"content\": \"否\"}\n",
    "                    ]\n",
    "                }\n",
    "                negative_samples.append(sample)\n",
    "            \n",
    "            if len(negative_samples) >= negative_count_needed:\n",
    "                break\n",
    "        \n",
    "        samples.extend(negative_samples)\n",
    "        return samples\n",
    "    \n",
    "    def build_dataset(self, train_ratio: float = 0.8):\n",
    "        \"\"\"\n",
    "        构建完整数据集\n",
    "        \n",
    "        Args:\n",
    "            train_ratio: 训练集占全部txt文件的比例\n",
    "        \"\"\"\n",
    "        # 获取所有txt文件\n",
    "        txt_files = list(Path(self.txt_folder_path).glob(\"*.txt\"))\n",
    "        \n",
    "        if not txt_files:\n",
    "            raise ValueError(f\"在 {self.txt_folder_path} 中未找到txt文件\")\n",
    "        # print(f\"找到 {len(txt_files)} 个txt文件\")\n",
    "        \n",
    "        # 按文件大小排序\n",
    "        txt_files.sort(key=lambda x: x.stat().st_size)\n",
    "        \n",
    "        # 划分训练集和测试集\n",
    "        split_index = int(len(txt_files) * train_ratio)\n",
    "        train_files = txt_files[:split_index]\n",
    "        test_files = txt_files[split_index:]\n",
    "        \n",
    "        print(f\"训练集文件数: {len(train_files)}\")\n",
    "        print(f\"测试集文件数: {len(test_files)}\")\n",
    "        \n",
    "        # 生成训练集\n",
    "        train_samples = []\n",
    "        for file_path in train_files:\n",
    "            print(f\"处理训练文件: {file_path.name}\")\n",
    "            try:\n",
    "                file_data = self.parse_txt_file(str(file_path))\n",
    "                samples = self.generate_samples_from_file(file_data)\n",
    "                train_samples.extend(samples)\n",
    "                print(f\"  生成样本数: {len(samples)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  处理文件 {file_path.name} 时出错: {e}\")\n",
    "        \n",
    "        # 生成测试集\n",
    "        test_samples = []\n",
    "        for file_path in test_files:\n",
    "            print(f\"处理测试文件: {file_path.name}\")\n",
    "            try:\n",
    "                file_data = self.parse_txt_file(str(file_path))\n",
    "                samples = self.generate_samples_from_file(file_data)\n",
    "                test_samples.extend(samples)\n",
    "                print(f\"  生成样本数: {len(samples)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  处理文件 {file_path.name} 时出错: {e}\")\n",
    "        \n",
    "        # 打乱样本顺序\n",
    "        random.shuffle(train_samples)\n",
    "        random.shuffle(test_samples)\n",
    "        \n",
    "        # 保存数据集\n",
    "        train_path = os.path.join(self.output_path, \"train.json\")\n",
    "        test_path = os.path.join(self.output_path, \"test.json\")\n",
    "        \n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_samples, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(test_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_samples, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # 统计信息\n",
    "        train_positive = sum(1 for s in train_samples if s[\"messages\"][2][\"content\"] == \"是\")\n",
    "        train_negative = len(train_samples) - train_positive\n",
    "        test_positive = sum(1 for s in test_samples if s[\"messages\"][2][\"content\"] == \"是\")\n",
    "        test_negative = len(test_samples) - test_positive\n",
    "        \n",
    "        print(\"\\n=== 数据集构建完成 ===\")\n",
    "        print(f\"训练集总样本数: {len(train_samples)}\")\n",
    "        print(f\"  正样本(广告): {train_positive}\")\n",
    "        print(f\"  负样本(非广告): {train_negative}\")\n",
    "        print(f\"测试集总样本数: {len(test_samples)}\")\n",
    "        print(f\"  正样本(广告): {test_positive}\")\n",
    "        print(f\"  负样本(非广告): {test_negative}\")\n",
    "        print(f\"训练集保存至: {train_path}\")\n",
    "        print(f\"测试集保存至: {test_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "015e853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集文件数: 30\n",
      "测试集文件数: 8\n",
      "处理训练文件: 南娘大赛3.0质量越来越顶了,主播依旧完败 [BV1r8bNzuEeL].txt\n",
      "  生成样本数: 25\n",
      "处理训练文件: 你永远不知道一只舔狗在背后付出多少努力(Av1301840966,P1).txt\n",
      "  生成样本数: 85\n",
      "处理训练文件: 不瞒你说，我在男厕邂逅了我的真爱(Av113741588858315,P1).txt\n",
      "  生成样本数: 39\n",
      "处理训练文件: 大英警察耻辱之战！小女警被阿拉伯猛男兄弟打到身体心理双重崩溃！(Av115032126065842,P1).txt\n",
      "  生成样本数: 30\n",
      "处理训练文件: 谁说离了房地产，中国经济就不行了？(Av114873346301283,P1).txt\n",
      "  生成样本数: 504\n",
      "处理训练文件: 俄军已攻入康斯坦丁尼夫卡！[俄乌沙盘318期](Av115065747411681,P1).txt\n",
      "  生成样本数: 182\n",
      "处理训练文件: 谁是造成年轻人电子阳痿的元凶？【丰言疯话】 [BV1Nhe9zhEmb].txt\n",
      "  生成样本数: 114\n",
      "处理训练文件: 什么炮弹切碎后性能更好？【军事榨菜34】(Av115020046274162,P1).txt\n",
      "  生成样本数: 150\n",
      "处理训练文件: 日本社会为何遍地“無敵の人”？【丰言疯话】(Av114957987355674,P1).txt\n",
      "  生成样本数: 189\n",
      "处理训练文件: 俄军攻占克列班比克，塞雷德涅，菲利亚！[俄乌沙盘319期](Av115088480539723,P1).txt\n",
      "  生成样本数: 175\n",
      "处理训练文件: BBC打脸！ 大英国家级先进工程叕是中国制造(Av114896297659189,P1).txt\n",
      "  生成样本数: 316\n",
      "处理训练文件: 【睡前消息932】杭州水污染 慢一步的真相追不上谣言(Av114936445408060,P1).txt\n",
      "  生成样本数: 264\n",
      "处理训练文件: 福建舰的电磁弹射领先在哪？【军事榨菜28】(Av114506210479676,P1).txt\n",
      "  生成样本数: 514\n",
      "处理训练文件: 【睡前消息926】4个古城没水喝，我们有1000个(Av114856904629733,P1).txt\n",
      "  生成样本数: 317\n",
      "处理训练文件: 【睡前消息944】雅鲁藏布江电站 造台阶不造滑梯(Av115094922993645,P1).txt\n",
      "  生成样本数: 287\n",
      "处理训练文件: “电竞高手”是怎样炼成的？【军事榨菜29】(Av114574275644601,P1).txt\n",
      "  生成样本数: 311\n",
      "处理训练文件: 俄乌战线3年来最大突破！小泽拒绝川普停战方案！[俄乌沙盘315期](Av115009157862729,P1).txt\n",
      "  生成样本数: 207\n",
      "处理训练文件: 【睡前消息941】沙特去石油化  假打才能保命(Av115055144276181,P1).txt\n",
      "  生成样本数: 401\n",
      "处理训练文件: 美国内战，一个政治经济学的死亡诊断(Av114759546441298,P1).txt\n",
      "  生成样本数: 160\n",
      "处理训练文件: 为了保护“大蛋蛋”，总统竟因未成年犯罪派兵占领首都(Av115015113900203,P1).txt\n",
      "  生成样本数: 244\n",
      "处理训练文件: 95步枪抄袭？子弹杀人靠冲击波？【鉴定网络热门军事57】(Av114454217949313,P1).txt\n",
      "  生成样本数: 407\n",
      "处理训练文件: 俄军攻占金井村！乌军防线彻底崩溃？[俄乌沙盘316期](Av115026287400936,P1).txt\n",
      "  生成样本数: 290\n",
      "处理训练文件: 【睡前消息930】75公顷颐和园，1221个事业编(Av114913678991220,P1).txt\n",
      "  生成样本数: 281\n",
      "处理训练文件: 库普扬斯克战役全面启动！川普要求8.8停战！[俄乌沙盘314期](Av114986760281706,P1).txt\n",
      "  生成样本数: 245\n",
      "处理训练文件: 传统派还不认输：西瓜披萨与披萨间谍的妙妙故事(Av114998353331485,P1).txt\n",
      "  生成样本数: 549\n",
      "处理训练文件: 【睡前消息943】打胜仗年年有 嘴认输头一回(Av115083531263819,P1).txt\n",
      "  生成样本数: 348\n",
      "处理训练文件: 为什么英年早逝的科研人员越来越多？(Av115065613194952,P1).txt\n",
      "  生成样本数: 387\n",
      "处理训练文件: 【睡前消息936】体检漏癌症不赔 只有保险公司能管(Av114992833634150,P1).txt\n",
      "  生成样本数: 406\n",
      "处理训练文件: 【睡前消息935】洪森子孙要当首相 只好卖掉他信女儿(Av114975972528821,P1).txt\n",
      "  生成样本数: 354\n",
      "处理训练文件: 都是男娘害的？男孩为何「雌堕」？(Av1552678062,P1).txt\n",
      "  生成样本数: 266\n",
      "处理测试文件: 日本人在招核日说“国破山河在”，杜甫的棺材板按不住了【毒舌的南瓜】(Av114991810219350,P1).txt\n",
      "  生成样本数: 322\n",
      "处理测试文件: 让台湾复刻“西安事变2.0”？新时代网民，正在呼唤大汉赢学【毒舌的南瓜】(Av114900424788583,P1).txt\n",
      "  生成样本数: 372\n",
      "处理测试文件: 不婚不育=进步？家庭解体=妇女解放？到底是什么在压迫女性？(Av1601585395,P1).txt\n",
      "  生成样本数: 497\n",
      "处理测试文件: 【睡前消息924】进步不靠谱 《大而美法案》就来了(Av114834943381557,P1).txt\n",
      "  生成样本数: 514\n",
      "处理测试文件: “中东特朗普”，是如何成为“元首”的？【毒舌的南瓜】(Av114862139179259,P1).txt\n",
      "  生成样本数: 420\n",
      "处理测试文件: 新二十一条惹争议，一夫一妻制走向消亡？新家庭又是什么样？【核心家庭变迁史】(Av1354084064,P1).txt\n",
      "  生成样本数: 344\n",
      "处理测试文件: 【睡前消息928】造红楼梦城、豪赌核聚变，居然是同一家公司？(Av114884872313989,P1).txt\n",
      "  生成样本数: 294\n",
      "处理测试文件: 【睡前消息933】学中医 给马督工诊脉，当律师 被患者“打黑”(Av114952853527656,P1).txt\n",
      "  生成样本数: 583\n",
      "\n",
      "=== 数据集构建完成 ===\n",
      "训练集总样本数: 8047\n",
      "  正样本(广告): 6260\n",
      "  负样本(非广告): 1787\n",
      "测试集总样本数: 3346\n",
      "  正样本(广告): 2359\n",
      "  负样本(非广告): 987\n",
      "训练集保存至: ./sft_dataset/train.json\n",
      "测试集保存至: ./sft_dataset/test.json\n"
     ]
    }
   ],
   "source": [
    "# 配置参数\n",
    "txt_folder_path = \"/mnt/e/LLM/output/done\"  # 替换为你的txt文件文件夹路径\n",
    "output_path = \"./sft_dataset\"    # 输出路径\n",
    "\n",
    "# 创建数据集构建器\n",
    "builder = SFTDatasetBuilder(txt_folder_path, output_path)\n",
    "\n",
    "# 可调整的参数\n",
    "builder.context_min_length = 50    # context最小长度\n",
    "builder.context_max_length = 200   # context最大长度\n",
    "builder.text_min_length = 30        # text最小长度\n",
    "builder.text_max_length = 100       # text最大长度\n",
    "builder.positive_negative_ratio = 1.0  # 正负样本比例\n",
    "\n",
    "# 构建数据集\n",
    "builder.build_dataset(train_ratio=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funasr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
